#!/usr/bin/env python3
"""
Independent Validation Scripts for Testing Review Results

This script demonstrates exactly how to use the generated raw data files
to independently reproduce and validate the statistical claims.

Usage:
    python3 independent_validation_demo.py

This script loads the raw numpy arrays and JSON files generated by 
comprehensive_validation.py and shows step-by-step how to recompute
all the statistical measures mentioned in the testing review.
"""

import numpy as np
import json
from scipy import stats
from scipy.stats import ks_2samp
import pandas as pd
import os

def load_and_validate_correlation():
    """
    Recompute Pearson correlation exactly as requested in the review
    """
    print("=== 1) Recompute Pearson correlation & CI ===")
    
    # Load the correlation data
    with open('../validation_output/correlation_data.json', 'r') as f:
        data = json.load(f)
    
    a = np.array(data['array_a'])
    b = np.array(data['array_b'])
    
    print(f"Array lengths: a={len(a)}, b={len(b)}")
    print(f"Array a sample: {a[:5]}")
    print(f"Array b sample: {b[:5]}")
    
    # Recompute correlation
    r, p = stats.pearsonr(a, b)
    print(f"Recomputed Pearson r = {r:.4f}, p = {p:.4e}")
    
    # Bootstrap CI (as requested in review)
    print("Computing bootstrap CI (10000 resamples)...")
    boots = []
    n = len(a)
    np.random.seed(42)  # For reproducibility
    
    for _ in range(10000):
        idx = np.random.randint(0, n, n)
        r_boot, _ = stats.pearsonr(a[idx], b[idx])
        boots.append(r_boot)
    
    ci = np.percentile(boots, [2.5, 97.5])
    print(f"Bootstrap 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
    
    # Compare with stored results
    stored_r = data['correlation']
    stored_ci = data['confidence_interval']
    print(f"Stored r = {stored_r:.4f}, CI = [{stored_ci[0]:.4f}, {stored_ci[1]:.4f}]")
    print(f"Validation: {'‚úì MATCH' if abs(r - stored_r) < 1e-10 else '‚úó MISMATCH'}")
    print()

def load_and_validate_ks_test():
    """
    Recompute KS statistic exactly as requested in the review
    """
    print("=== 2) Recompute KS statistic ===")
    
    # Load the arrays used for KS test
    prime_vals = np.load('../validation_output/prime_chiral_distances.npy')
    composite_vals = np.load('../validation_output/composite_chiral_distances.npy')
    
    print(f"Prime values: n={len(prime_vals)}, sample={prime_vals[:5]}")
    print(f"Composite values: n={len(composite_vals)}, sample={composite_vals[:5]}")
    
    # Recompute KS test
    ks_stat, ks_p = ks_2samp(prime_vals, composite_vals)
    print(f"KS statistic = {ks_stat:.4f}, p = {ks_p:.4e}")
    
    # Additional statistics
    print(f"Prime mean = {np.mean(prime_vals):.4f}, std = {np.std(prime_vals):.4f}")
    print(f"Composite mean = {np.mean(composite_vals):.4f}, std = {np.std(composite_vals):.4f}")
    print()

def load_and_validate_cohens_d():
    """
    Compute Cohen's d effect size as requested in the review
    """
    print("=== 3) Compute Cohen's d effect size ===")
    
    # Load the chiral data
    prime_chiral = np.load('../validation_output/prime_chiral_distances.npy')
    composite_chiral = np.load('../validation_output/composite_chiral_distances.npy')
    
    # Cohen's d calculation exactly as specified in the review
    def cohens_d(x, y):
        nx, ny = len(x), len(y)
        s = np.sqrt(((nx-1)*x.std(ddof=1)**2 + (ny-1)*y.std(ddof=1)**2)/(nx+ny-2))
        return (x.mean()-y.mean())/s
    
    d = cohens_d(prime_chiral, composite_chiral)
    print(f"Cohen's d = {d:.4f}")
    
    # Effect size interpretation
    if abs(d) < 0.2:
        interpretation = "negligible"
    elif abs(d) < 0.5:
        interpretation = "small"
    elif abs(d) < 0.8:
        interpretation = "medium"
    else:
        interpretation = "large"
    
    print(f"Effect size interpretation: {interpretation}")
    print()

def demonstrate_k_parameter_analysis():
    """
    Show the k* parameter search results and multiple testing considerations
    """
    print("=== 4) K* parameter analysis & multiple testing ===")
    
    # Load k-sweep data
    k_values = np.load('../validation_output/k_values.npy')
    max_enhancements = np.load('../validation_output/max_enhancements.npy')
    gmm_sigmas = np.load('../validation_output/gmm_sigmas.npy')
    fourier_sums = np.load('../validation_output/fourier_sums.npy')
    
    print(f"K parameter range: [{k_values[0]:.3f}, {k_values[-1]:.3f}]")
    print(f"Number of k values tested: {len(k_values)}")
    
    # Find optimal k*
    best_idx = np.argmax(max_enhancements)
    k_star = k_values[best_idx]
    max_enh = max_enhancements[best_idx]
    
    print(f"Optimal k* = {k_star:.3f}")
    print(f"Maximum enhancement = {max_enh:.1f}%")
    
    # Show top 5 k values
    sorted_indices = np.argsort(max_enhancements)[::-1]
    print("\nTop 5 k values by enhancement:")
    for i in range(min(5, len(sorted_indices))):
        idx = sorted_indices[i]
        print(f"  k={k_values[idx]:.3f}: {max_enhancements[idx]:.1f}% enhancement")
    
    # Multiple testing warning
    print(f"\n‚ö†Ô∏è  Multiple testing warning:")
    print(f"   {len(k_values)} k values were tested.")
    print(f"   P-values should be corrected (e.g., Bonferroni: Œ± = 0.05/{len(k_values)} = {0.05/len(k_values):.6f})")
    print(f"   Or use permutation test as implemented in comprehensive validation.")
    print()

def demonstrate_permutation_test():
    """
    Show how to run a permutation test as requested in the review
    """
    print("=== 5) Permutation test demonstration ===")
    
    # Load curvature data
    prime_vals = np.load('../validation_output/prime_curvature_values.npy')
    composite_vals = np.load('../validation_output/composite_curvature_values.npy')
    
    # Observed difference in means
    observed_diff = np.mean(prime_vals) - np.mean(composite_vals)
    print(f"Observed difference in means: {observed_diff:.4f}")
    
    # Permutation test
    all_vals = np.concatenate([prime_vals, composite_vals])
    n_prime = len(prime_vals)
    n_permutations = 1000
    
    permuted_diffs = []
    np.random.seed(42)
    
    for _ in range(n_permutations):
        # Shuffle labels
        shuffled = np.random.permutation(all_vals)
        perm_prime = shuffled[:n_prime]
        perm_composite = shuffled[n_prime:]
        
        # Compute difference
        perm_diff = np.mean(perm_prime) - np.mean(perm_composite)
        permuted_diffs.append(perm_diff)
    
    # Compute p-value
    p_value = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff))
    
    print(f"Permutation test (n={n_permutations}):")
    print(f"  P-value = {p_value:.4f}")
    print(f"  Null distribution mean = {np.mean(permuted_diffs):.4f}")
    print(f"  Null distribution std = {np.std(permuted_diffs):.4f}")
    print()

def generate_summary_statistics():
    """
    Generate comprehensive summary statistics for all arrays
    """
    print("=== 6) Summary statistics for all raw data ===")
    
    files_to_analyze = [
        ('prime_curvature_values.npy', 'Prime curvature values'),
        ('composite_curvature_values.npy', 'Composite curvature values'),
        ('prime_chiral_distances.npy', 'Prime chiral distances'),
        ('composite_chiral_distances.npy', 'Composite chiral distances'),
        ('zeta_spacing_unfolded.npy', 'Zeta spacing (unfolded)'),
        ('max_enhancements.npy', 'Maximum enhancements'),
    ]
    
    summary_data = []
    
    for filename, description in files_to_analyze:
        try:
            data = np.load(f'../validation_output/{filename}')
            stats_dict = {
                'Dataset': description,
                'N': len(data),
                'Mean': f"{np.mean(data):.4f}",
                'Std': f"{np.std(data):.4f}",
                'Min': f"{np.min(data):.4f}",
                'Max': f"{np.max(data):.4f}",
                'Median': f"{np.median(data):.4f}"
            }
            summary_data.append(stats_dict)
            
        except FileNotFoundError:
            print(f"Warning: {filename} not found")
    
    # Create summary table
    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))
    
    # Save summary
    summary_df.to_csv('../validation_output/raw_data_summary.csv', index=False)
    print(f"\nSummary saved to: ../validation_output/raw_data_summary.csv")
    print()

def create_reproducibility_code():
    """
    Create exact code snippets for independent validation
    """
    print("=== 7) Reproducibility code snippets ===")
    
    code_snippets = {
        "Load correlation arrays": """
import numpy as np
import json
from scipy import stats

# Load correlation data
with open('../validation_output/correlation_data.json', 'r') as f:
    data = json.load(f)
a = np.array(data['array_a'])
b = np.array(data['array_b'])

# Compute correlation
r, p = stats.pearsonr(a, b)
print(f"r = {r:.4f}, p = {p:.4e}")
""",
        
        "KS test": """
import numpy as np
from scipy.stats import ks_2samp

# Load arrays
prime_vals = np.load('../validation_output/prime_chiral_distances.npy')
composite_vals = np.load('../validation_output/composite_chiral_distances.npy')

# KS test
ks_stat, p = ks_2samp(prime_vals, composite_vals)
print(f"KS stat = {ks_stat:.4f}, p = {p:.4e}")
""",
        
        "Bootstrap confidence interval": """
import numpy as np
from scipy import stats

# Assuming arrays a and b are loaded
boots = []
n = len(a)
for _ in range(10000):
    idx = np.random.randint(0, n, n)
    r_boot, _ = stats.pearsonr(a[idx], b[idx])
    boots.append(r_boot)

ci = np.percentile(boots, [2.5, 97.5])
print(f"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
""",
        
        "Cohen's d": """
import numpy as np

def cohens_d(x, y):
    nx, ny = len(x), len(y)
    s = np.sqrt(((nx-1)*x.std(ddof=1)**2 + (ny-1)*y.std(ddof=1)**2)/(nx+ny-2))
    return (x.mean()-y.mean())/s

# Load data and compute
x = np.load('../validation_output/prime_chiral_distances.npy')
y = np.load('../validation_output/composite_chiral_distances.npy')
d = cohens_d(x, y)
print(f"Cohen's d = {d:.4f}")
"""
    }
    
    # Save code snippets
    with open('../validation_output/reproducibility_code.py', 'w') as f:
        f.write("#!/usr/bin/env python3\n")
        f.write('"""\nReproducibility Code Snippets\n\n')
        f.write("Code to independently validate all statistical claims\n")
        f.write('"""\n\n')
        
        for title, code in code_snippets.items():
            f.write(f"# {title}\n")
            f.write("#" + "="*50 + "\n")
            f.write(code)
            f.write("\n\n")
    
    print("Code snippets saved to: ../validation_output/reproducibility_code.py")
    print()
    
    for title, code in code_snippets.items():
        print(f"### {title}")
        print("```python")
        print(code.strip())
        print("```")
        print()

def main():
    print("üî¨ Independent Validation Demonstration")
    print("=" * 50)
    print("This script demonstrates how to use the generated raw data")
    print("to independently reproduce all statistical claims.")
    print()
    
    # Check if validation output exists
    if not os.path.exists('../validation_output'):
        print("‚ùå Error: ../validation_output directory not found!")
        print("Please run comprehensive_validation.py first.")
        return
    
    try:
        # Run all validation demonstrations
        load_and_validate_correlation()
        load_and_validate_ks_test()
        load_and_validate_cohens_d()
        demonstrate_k_parameter_analysis()
        demonstrate_permutation_test()
        generate_summary_statistics()
        create_reproducibility_code()
        
        print("‚úÖ All validation demonstrations completed!")
        print("\nFiles available for independent verification:")
        files = [f for f in os.listdir('../validation_output') if f.endswith(('.npy', '.json', '.csv'))]
        for f in sorted(files):
            print(f"  üìÅ ../validation_output/{f}")
        
    except Exception as e:
        print(f"‚ùå Error during validation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()