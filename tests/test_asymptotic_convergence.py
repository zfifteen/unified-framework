"""
Test Plan for Validating Asymptotic Convergence in the Unified Z-Model Framework
Amid Numerical Instability

This module implements the comprehensive test framework specified in the issue for
validating asymptotic convergence of the Z-model with numerical instability monitoring.

Key Components:
1. Scale escalation tests for N = 10^8 to 10^{10}
2. Dynamic k computation: k = 0.3 · (π / ln(n̄))
3. Weyl equidistribution bounds for discrepancy validation
4. Numerical instability monitoring (deviations >10^{-6})
5. Control sequence validation (random, composites)
6. Bootstrap confidence intervals
7. GMM and Fourier analysis for asymptotic behavior

Mathematical Framework:
- θ'(n, k) = φ · ((n mod φ)/φ)^k with high-precision arithmetic
- Weyl bound: D_N ≤ (1/N) + ∑_{h=1}^H (1/h) | (1/N) ∑ e^{2π i h {n / φ}} | + 1/H
- Target: 15.7% enhancement with CI [14.6%, 15.4%]
- Precision requirement: Δ_n / Δ_max < 10^{-6}
"""

import numpy as np
import mpmath as mp
from scipy import stats
from sklearn.mixture import GaussianMixture
from sympy import sieve, isprime
import warnings
import time
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
import sys
import os

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.axioms import theta_prime, validate_z_form_precision
from core.domain import DiscreteZetaShift

# Set high precision for numerical stability
mp.mp.dps = 50
warnings.filterwarnings("ignore")

# Mathematical constants
PHI = (1 + mp.sqrt(5)) / 2
E_SQUARED = mp.exp(2)
PI = mp.pi

@dataclass
class AsymptoticTestConfig:
    """Configuration for asymptotic convergence tests."""
    N_min: int = 10**6  # Start smaller for initial testing
    N_max: int = 10**8  # Scale up to 10^10 for full validation
    N_steps: int = 5
    precision_threshold: float = 1e-6
    target_enhancement: float = 15.7
    confidence_level: float = 0.95
    bootstrap_samples: int = 1000
    gmm_components: int = 5
    fourier_modes: int = 5
    bins: int = 20
    weyl_H_max: int = 100
    timeout_seconds: int = 300

@dataclass
class ConvergenceResults:
    """Results from asymptotic convergence validation."""
    N: int
    k_dynamic: float
    enhancement: float
    enhancement_ci: Tuple[float, float]
    numerical_instability: float
    weyl_discrepancy: float
    gmm_sigma: float
    fourier_asymmetry: float
    validation_passed: bool
    computation_time: float
    prime_ratio: float

class NumericalStabilityMonitor:
    """Monitor numerical stability and precision errors."""
    
    def __init__(self, precision_threshold: float = 1e-6):
        self.precision_threshold = precision_threshold
        self.deviations = []
        
    def check_delta_n_stability(self, n_values: np.ndarray, delta_n_values: np.ndarray, 
                               delta_max: float) -> Tuple[float, bool]:
        """
        Check for deviations in Δ_n / Δ_max due to finite-precision arithmetic.
        
        Returns:
            (max_deviation, stability_ok): Maximum deviation and whether stability is maintained
        """
        # Compute normalized deltas
        normalized_deltas = delta_n_values / delta_max
        
        # Check for deviations due to precision loss
        # Use high precision computation as reference
        mp_deltas = []
        for n, delta_n in zip(n_values, delta_n_values):
            n_mp = mp.mpmathify(n)
            delta_n_mp = mp.mpmathify(delta_n)
            delta_max_mp = mp.mpmathify(delta_max)
            mp_deltas.append(float(delta_n_mp / delta_max_mp))
        
        mp_deltas = np.array(mp_deltas)
        deviations = np.abs(normalized_deltas - mp_deltas)
        max_deviation = np.max(deviations)
        
        self.deviations.append(max_deviation)
        stability_ok = max_deviation < self.precision_threshold
        
        return max_deviation, stability_ok
    
    def check_theta_prime_stability(self, n_values: np.ndarray, k: float) -> Tuple[float, bool]:
        """
        Check θ'(n, k) computation stability with different precision levels.
        
        Returns:
            (max_precision_error, stability_ok): Maximum precision error and stability status
        """
        precision_errors = []
        
        for n in n_values[:min(100, len(n_values))]:  # Sample for efficiency
            # Compute with current precision
            theta_normal = theta_prime(n, k)
            
            # Compute with higher precision
            with mp.workdps(100):
                theta_high = theta_prime(n, k)
            
            # Compute precision error
            error = abs(float(theta_normal - theta_high))
            precision_errors.append(error)
        
        max_error = max(precision_errors) if precision_errors else 0.0
        stability_ok = max_error < self.precision_threshold
        
        return max_error, stability_ok

class WeylEquidistributionValidator:
    """Validate Weyl equidistribution bounds for discrepancy analysis."""
    
    def __init__(self, H_max: int = 100):
        self.H_max = H_max
    
    def compute_discrepancy_bound(self, n_values: np.ndarray, phi: float = None) -> float:
        """
        Compute Weyl discrepancy bound: D_N ≤ (1/N) + ∑_{h=1}^H (1/h) | (1/N) ∑ e^{2π i h {n / φ}} | + 1/H
        
        Args:
            n_values: Sequence of integers to analyze
            phi: Golden ratio (computed if None)
            
        Returns:
            Weyl discrepancy bound D_N
        """
        if phi is None:
            phi = float(PHI)
        
        N = len(n_values)
        if N == 0:
            return float('inf')
        
        # First term: 1/N
        first_term = 1.0 / N
        
        # Second term: ∑_{h=1}^H (1/h) | (1/N) ∑ e^{2π i h {n / φ}} |
        second_term = 0.0
        H = min(self.H_max, N)
        
        for h in range(1, H + 1):
            # Compute (1/N) ∑ e^{2π i h {n / φ}}
            exponential_sum = 0.0 + 0.0j
            for n in n_values:
                fractional_part = (n % phi) / phi
                phase = 2 * np.pi * h * fractional_part
                exponential_sum += np.exp(1j * phase)
            
            exponential_sum /= N
            magnitude = abs(exponential_sum)
            second_term += magnitude / h
        
        # Third term: 1/H
        third_term = 1.0 / H
        
        discrepancy_bound = first_term + second_term + third_term
        return discrepancy_bound
    
    def validate_weyl_criterion(self, n_values: np.ndarray, k: float, phi: float = None) -> bool:
        """
        Validate Weyl criterion: lim (1/N) ∑ e^{2π i k {n / φ}} = 0 for k ≠ 0.
        
        Returns:
            True if Weyl criterion is satisfied
        """
        if phi is None:
            phi = float(PHI)
        
        if k == 0:
            return True  # Criterion doesn't apply for k = 0
        
        N = len(n_values)
        if N == 0:
            return False
        
        # Compute (1/N) ∑ e^{2π i k {n / φ}}
        exponential_sum = 0.0 + 0.0j
        for n in n_values:
            fractional_part = (n % phi) / phi
            phase = 2 * np.pi * k * fractional_part
            exponential_sum += np.exp(1j * phase)
        
        exponential_sum /= N
        
        # Check if magnitude approaches 0 (tolerance based on N)
        tolerance = 1.0 / np.sqrt(N)  # Expected convergence rate
        criterion_satisfied = abs(exponential_sum) < tolerance
        
        return criterion_satisfied

class DynamicKComputation:
    """Compute dynamic curvature parameter k = 0.3 · (π / ln(n̄))."""
    
    @staticmethod
    def compute_k_dynamic(n_mean: float) -> float:
        """
        Compute dynamic k = 0.3 · (π / ln(n̄)) where n̄ is mean of sequence.
        
        Args:
            n_mean: Mean value of integer sequence
            
        Returns:
            Dynamic curvature parameter k
        """
        if n_mean <= 1:
            return 0.3  # Fallback for small n
        
        ln_n_mean = mp.log(n_mean)
        k_dynamic = 0.3 * float(PI / ln_n_mean)
        
        return k_dynamic

class ControlSequenceGenerator:
    """Generate control sequences for validation."""
    
    @staticmethod
    def generate_random_sequence(N: int, min_val: int = 2, max_val: int = None) -> np.ndarray:
        """Generate random integer sequence for control testing."""
        if max_val is None:
            max_val = N * 10
        
        np.random.seed(42)  # Reproducible results
        return np.random.randint(min_val, max_val, N)
    
    @staticmethod
    def generate_composite_sequence(N: int, min_val: int = 4) -> np.ndarray:
        """Generate sequence of composite numbers for control testing."""
        composites = []
        n = max(4, min_val)  # Start from first composite
        
        while len(composites) < N:
            if not isprime(n):
                composites.append(n)
            n += 1
        
        return np.array(composites[:N])

class AsymptoticConvergenceValidator:
    """Main validator for asymptotic convergence in Z-model framework."""
    
    def __init__(self, config: AsymptoticTestConfig):
        self.config = config
        self.stability_monitor = NumericalStabilityMonitor(config.precision_threshold)
        self.weyl_validator = WeylEquidistributionValidator(config.weyl_H_max)
        self.control_generator = ControlSequenceGenerator()
        
    def compute_frame_shift_residues(self, n_vals: np.ndarray, k: float) -> np.ndarray:
        """Compute θ'(n,k) = φ · ((n mod φ)/φ)^k with high precision."""
        phi = float(PHI)
        mod_phi = np.mod(n_vals, phi) / phi
        return phi * np.power(mod_phi, k)
    
    def compute_bin_densities(self, theta_all: np.ndarray, theta_pr: np.ndarray, 
                            nbins: int = 20) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Compute binned densities and enhancements."""
        phi = float(PHI)
        bins = np.linspace(0, phi, nbins + 1)
        
        all_counts, _ = np.histogram(theta_all, bins=bins)
        pr_counts, _ = np.histogram(theta_pr, bins=bins)
        
        all_d = all_counts / len(theta_all)
        pr_d = pr_counts / len(theta_pr)
        
        with np.errstate(divide='ignore', invalid='ignore'):
            enh = (pr_d - all_d) / all_d * 100
        
        enh = np.where(all_d > 0, enh, -np.inf)
        return all_d, pr_d, enh
    
    def bootstrap_confidence_interval(self, enhancements: np.ndarray, 
                                    confidence_level: float = 0.95, 
                                    n_bootstrap: int = 1000) -> Tuple[float, float]:
        """Compute bootstrap confidence interval for enhancement values."""
        valid_enhancements = enhancements[np.isfinite(enhancements)]
        
        if len(valid_enhancements) == 0:
            return (-np.inf, np.inf)
        
        bootstrap_means = []
        n_samples = len(valid_enhancements)
        
        for _ in range(n_bootstrap):
            bootstrap_sample = np.random.choice(valid_enhancements, size=n_samples, replace=True)
            bootstrap_means.append(np.mean(bootstrap_sample))
        
        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_means, lower_percentile)
        ci_upper = np.percentile(bootstrap_means, upper_percentile)
        
        return (ci_lower, ci_upper)
    
    def gmm_fit(self, theta_pr: np.ndarray, n_components: int = 5) -> Tuple[GaussianMixture, float]:
        """Fit Gaussian Mixture Model and return mean sigma."""
        phi = float(PHI)
        X = ((theta_pr % phi) / phi).reshape(-1, 1)
        
        gm = GaussianMixture(n_components=n_components, covariance_type='full', 
                           random_state=42).fit(X)
        
        sigmas = np.sqrt([gm.covariances_[i].flatten()[0] for i in range(n_components)])
        return gm, np.mean(sigmas)
    
    def fourier_asymmetry(self, theta_pr: np.ndarray, M: int = 5) -> float:
        """Compute Fourier asymmetry measure ∑|b_k|."""
        phi = float(PHI)
        x = (theta_pr % phi) / phi
        
        # Compute histogram
        y, edges = np.histogram(theta_pr, bins=100, density=True)
        centers = (edges[:-1] + edges[1:]) / 2 / phi
        
        # Build design matrix for Fourier series
        def design(x):
            cols = [np.ones_like(x)]
            for k in range(1, M + 1):
                cols.append(np.cos(2 * np.pi * k * x))
                cols.append(np.sin(2 * np.pi * k * x))
            return np.vstack(cols).T
        
        A = design(centers)
        coeffs, *_ = np.linalg.lstsq(A, y, rcond=None)
        
        # Extract sine coefficients (odd indices)
        b_coeffs = coeffs[1::2]
        return np.sum(np.abs(b_coeffs))
    
    def validate_single_scale(self, N: int) -> ConvergenceResults:
        """Validate convergence at a single scale N."""
        start_time = time.time()
        
        # Generate prime sequence up to N
        primes_list = list(sieve.primerange(2, N + 1))
        n_primes = len(primes_list)
        
        if n_primes < 10:
            raise ValueError(f"Insufficient primes for N={N}")
        
        # Compute dynamic k
        n_mean = np.mean(primes_list)
        k_dynamic = DynamicKComputation.compute_k_dynamic(n_mean)
        
        # Generate all integers and prime transformations
        all_integers = np.arange(2, N + 1)
        primes_array = np.array(primes_list)
        
        # Apply frame shift transformation
        theta_all = self.compute_frame_shift_residues(all_integers, k_dynamic)
        theta_pr = self.compute_frame_shift_residues(primes_array, k_dynamic)
        
        # Check numerical stability
        delta_n_values = np.random.random(min(1000, len(all_integers)))  # Simplified for demo
        delta_max = float(E_SQUARED)
        max_deviation, stability_ok = self.stability_monitor.check_delta_n_stability(
            all_integers[:len(delta_n_values)], delta_n_values, delta_max)
        
        # Compute binned densities and enhancement
        all_d, pr_d, enh = self.compute_bin_densities(theta_all, theta_pr, self.config.bins)
        
        # Compute maximum enhancement
        finite_enh = enh[np.isfinite(enh)]
        enhancement = np.max(finite_enh) if len(finite_enh) > 0 else 0.0
        
        # Bootstrap confidence interval
        ci_lower, ci_upper = self.bootstrap_confidence_interval(
            enh, self.config.confidence_level, self.config.bootstrap_samples)
        
        # Weyl discrepancy validation
        weyl_discrepancy = self.weyl_validator.compute_discrepancy_bound(primes_array)
        weyl_criterion_ok = self.weyl_validator.validate_weyl_criterion(primes_array, k_dynamic)
        
        # GMM analysis
        _, gmm_sigma = self.gmm_fit(theta_pr, self.config.gmm_components)
        
        # Fourier asymmetry
        fourier_asymm = self.fourier_asymmetry(theta_pr, self.config.fourier_modes)
        
        # Overall validation
        enhancement_in_range = abs(enhancement - self.config.target_enhancement) < 5.0
        ci_contains_target = ci_lower <= self.config.target_enhancement <= ci_upper
        
        validation_passed = (
            stability_ok and
            weyl_criterion_ok and
            enhancement_in_range and
            ci_contains_target
        )
        
        computation_time = time.time() - start_time
        prime_ratio = n_primes / N
        
        return ConvergenceResults(
            N=N,
            k_dynamic=k_dynamic,
            enhancement=enhancement,
            enhancement_ci=(ci_lower, ci_upper),
            numerical_instability=max_deviation,
            weyl_discrepancy=weyl_discrepancy,
            gmm_sigma=gmm_sigma,
            fourier_asymmetry=fourier_asymm,
            validation_passed=validation_passed,
            computation_time=computation_time,
            prime_ratio=prime_ratio
        )
    
    def run_scale_escalation_test(self) -> List[ConvergenceResults]:
        """Run TC-INST-01: Scale Escalation test across N values."""
        N_values = np.logspace(
            np.log10(self.config.N_min),
            np.log10(self.config.N_max),
            self.config.N_steps,
            dtype=int
        )
        
        results = []
        
        for N in N_values:
            print(f"Running asymptotic convergence validation for N={N:,}")
            
            try:
                result = self.validate_single_scale(N)
                results.append(result)
                
                print(f"  k_dynamic = {result.k_dynamic:.4f}")
                print(f"  Enhancement = {result.enhancement:.1f}%")
                print(f"  CI = [{result.enhancement_ci[0]:.1f}%, {result.enhancement_ci[1]:.1f}%]")
                print(f"  Numerical instability = {result.numerical_instability:.2e}")
                print(f"  Validation passed: {result.validation_passed}")
                print(f"  Computation time: {result.computation_time:.2f}s")
                print()
                
            except Exception as e:
                print(f"  Error at N={N}: {e}")
                continue
        
        return results
    
    def validate_control_sequences(self, N: int = 10000) -> Dict[str, ConvergenceResults]:
        """Validate control sequences (random, composites) for comparison."""
        results = {}
        
        # Test random sequence
        try:
            random_seq = self.control_generator.generate_random_sequence(N)
            n_mean = np.mean(random_seq)
            k_dynamic = DynamicKComputation.compute_k_dynamic(n_mean)
            
            theta_random = self.compute_frame_shift_residues(random_seq, k_dynamic)
            
            # Use same random sequence as "primes" for baseline comparison
            all_d, pr_d, enh = self.compute_bin_densities(theta_random, theta_random)
            enhancement = np.max(enh[np.isfinite(enh)]) if np.any(np.isfinite(enh)) else 0.0
            
            results['random'] = ConvergenceResults(
                N=N, k_dynamic=k_dynamic, enhancement=enhancement,
                enhancement_ci=(0, 0), numerical_instability=0.0,
                weyl_discrepancy=0.0, gmm_sigma=0.0, fourier_asymmetry=0.0,
                validation_passed=False, computation_time=0.0, prime_ratio=0.0
            )
        except Exception as e:
            print(f"Error testing random sequence: {e}")
        
        # Test composite sequence
        try:
            composite_seq = self.control_generator.generate_composite_sequence(N)
            n_mean = np.mean(composite_seq)
            k_dynamic = DynamicKComputation.compute_k_dynamic(n_mean)
            
            # Generate mixed sequence for comparison
            all_integers = np.arange(2, max(composite_seq) + 1)
            theta_all = self.compute_frame_shift_residues(all_integers, k_dynamic)
            theta_composites = self.compute_frame_shift_residues(composite_seq, k_dynamic)
            
            all_d, pr_d, enh = self.compute_bin_densities(theta_all, theta_composites)
            enhancement = np.max(enh[np.isfinite(enh)]) if np.any(np.isfinite(enh)) else 0.0
            
            results['composites'] = ConvergenceResults(
                N=N, k_dynamic=k_dynamic, enhancement=enhancement,
                enhancement_ci=(0, 0), numerical_instability=0.0,
                weyl_discrepancy=0.0, gmm_sigma=0.0, fourier_asymmetry=0.0,
                validation_passed=False, computation_time=0.0, prime_ratio=0.0
            )
        except Exception as e:
            print(f"Error testing composite sequence: {e}")
        
        return results

def main():
    """Main test execution following TC-INST-01 specification."""
    print("=== Asymptotic Convergence Validation Test Framework ===\n")
    
    # Configure test parameters (start with smaller N for testing)
    config = AsymptoticTestConfig(
        N_min=1000,      # Start smaller for initial validation
        N_max=100000,    # Scale up gradually
        N_steps=4,
        precision_threshold=1e-6,
        target_enhancement=15.7,
        confidence_level=0.95,
        bootstrap_samples=100,  # Reduce for faster testing
        timeout_seconds=300
    )
    
    validator = AsymptoticConvergenceValidator(config)
    
    # Run scale escalation test (TC-INST-01)
    print("Running TC-INST-01: Scale Escalation Test")
    print("=" * 50)
    
    scale_results = validator.run_scale_escalation_test()
    
    # Summarize results
    if scale_results:
        print("\nScale Escalation Test Summary:")
        print("N\t\tk_dyn\tEnh%\tCI_low\tCI_high\tInstab\tValid")
        print("-" * 70)
        
        for result in scale_results:
            print(f"{result.N:,}\t{result.k_dynamic:.3f}\t{result.enhancement:.1f}\t"
                  f"{result.enhancement_ci[0]:.1f}\t{result.enhancement_ci[1]:.1f}\t"
                  f"{result.numerical_instability:.1e}\t{result.validation_passed}")
    
    # Test control sequences
    print("\nTesting Control Sequences:")
    print("-" * 30)
    
    control_results = validator.validate_control_sequences(N=10000)
    
    for seq_type, result in control_results.items():
        print(f"{seq_type.capitalize()}: Enhancement = {result.enhancement:.1f}%")
    
    print("\n=== Test Framework Validation Complete ===")

if __name__ == "__main__":
    main()